# general settings
name: LKMN_8blocks_36channels_1000k_DF2K_x3
model_type: SRModel
scale: 3
num_gpu: 1  # set num_gpu: 0 for cpu mode
manual_seed: 10
# python basicsr/train.py -opt options/train/train_LKMN_x3.yml --auto_resume
# dataset and data loader settings
datasets:
  train:
    name: DF2K
    type: PairedImageDataset
    #dataroot_gt: datasets/DIV2K/DIV2K_train_HR_sub
    #dataroot_lq: datasets/DIV2K/DIV2K_train_LR_bicubic/X3_sub
    # (for lmdb)
    dataroot_gt: datasets/DF2K/DF2K_HRmod12_sub.lmdb
    dataroot_lq: datasets/DF2K/DF2K_LRmod12x3_sub.lmdb
    filename_tmpl: '{}'
    io_backend:
      #type: disk
      # (for lmdb)
      type: lmdb

    gt_size: 144    # 48*48 LR input
    use_hflip: true
    use_rot: true

    # data loader
    num_worker_per_gpu: 14
    batch_size_per_gpu: 64
    dataset_enlarge_ratio: 10
    prefetch_mode: cuda
    pin_memory: true

  val:
    name: set5
    type: PairedImageDataset
    dataroot_gt: datasets/set5/mod3/GT
    dataroot_lq: datasets/set5/mod3/LRx3
    io_backend:
      type: disk


# network structures
network_g:
  type: LKMN
  in_channels: 3
  channels: 36
  out_channels: 3
  upscale: 3
  num_block: 8
  large_kernel: 31
  split_group: 4

# path
path:
  pretrain_network_g: ~
  strict_load_g: false
  resume_state: ~


# training settings
train:
 ema_decay: 0.999
 optim_g:
   type: Adan
   lr: !!float 5e-3
   betas: [0.98, 0.92, 0.99]
   weight_decay: 0
   foreach: true

 scheduler:
   type: CosineAnnealingRestartLR
   periods: [1000000]
   restart_weights: [1]
   eta_min: !!float 1e-6


 total_iter: 1000000
 warmup_iter: -1  # no warm up

 # losses
 pixel_opt:
   type: L1Loss
   loss_weight: 1.0
   reduction: mean

 fft_opt:
   type: FFTLoss
   loss_weight: 0.05
   reduction: mean

# validation settings
val:
 val_freq: !!float 5e3
 save_img: true
 self_ensemble_testing: false

 metrics:
   PSNR: # metric name, can be arbitrary
     type: calculate_psnr
     crop_border: 3
     test_y_channel: true
     better: higher
   SSIM:
     type: calculate_ssim
     crop_border: 3
     test_y_channel: true
     better: higher

# logging settings
logger:
 print_freq: 100
 save_checkpoint_freq: !!float 5e3
 use_tb_logger: true
 wandb:
   project: ~
   resume_id: ~

# dist training settings
dist_params:
 backend: nccl
 port: 29500
